# ğŸš€ Data Pipeline using DBT and Apache Airflow

## ğŸ“Œ Project Overview
This project sets up an **ELT (Extract, Load, Transform) pipeline** using **DBT (Data Build Tool) and Apache Airflow**. It integrates **Postgres, Docker, and DBT** to automate data workflows, orchestrate transformations, and manage dependencies efficiently.

## âœ¨ Features
-  **DBT for Data Transformation**: Define models, transformations, and dependencies.
-  **Apache Airflow for Orchestration**: Automate workflow execution.
-  **PostgreSQL as Data Warehouse**: Store raw and transformed data.
-  **Docker for Containerization**: Ensures environment consistency.
-  **ELT Architecture**: Extracts, loads, and transforms data efficiently.

## ğŸ“ File Structure
```
.
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ elt_dag.py               # ğŸš€ Airflow DAG for ELT pipeline
â”‚   â”‚   â””â”€â”€ airflow.cfg              # âš™ï¸ Airflow configuration
â”‚   â”œâ”€â”€ custom_postgres/
â”‚   â”‚   â”œâ”€â”€ macros/
â”‚   â”‚   â”‚   â”œâ”€â”€ film_ratings_macro.sql # ğŸ“Š SQL macros for DBT transformations
â”‚   â”‚   â”œâ”€â”€ models/example/
â”‚   â”‚   â”‚   â”œâ”€â”€ actors.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ film_actors.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ film_ratings.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ films.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ specific_movie.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ schema.yml
â”‚   â”‚   â”‚   â”œâ”€â”€ sources.yml
â”‚   â”‚   â”‚   â”œâ”€â”€ dbt_project.yml
â”œâ”€â”€ elt/
â”‚   â”œâ”€â”€ Dockerfile                    # ğŸ³ Docker configuration for ELT process
â”‚   â”œâ”€â”€ elt_script.py                  # ğŸ“ Python script to load data from source to destination
â”‚   â”œâ”€â”€ start.sh                        # ğŸš€ Shell script to initialize services
â”œâ”€â”€ source_db_init/
â”‚   â”œâ”€â”€ init.sql                        # ğŸ¯ SQL script to initialize source database
â”‚   â”œâ”€â”€ docker-compose.yml              # âš™ï¸ Docker Compose file for setting up the database
â”‚   â”œâ”€â”€ README.md                        # ğŸ“– Documentation for source database initialization
â”œâ”€â”€ README.md                           # ğŸ“ƒ Project documentation
```

## âš™ï¸ Setup & Installation
### ğŸ“Œ Prerequisites
-  Docker & Docker Compose
-  Python 3.8+
-  DBT installed
-  PostgreSQL

### ğŸš€ Steps to Run
1. **Clone the repository**
   ```sh
   git clone https://github.com/adityarathore622/Data-Pipeline-using-DBT-and-Apache-Airflow.git
   cd Data-Pipeline-using-DBT-and-Apache-Airflow/elt
   ```
2. **Start Docker services** 
   ```sh
   docker-compose up -d
   ```

3. **Access Airflow UI** 
   - Open `http://localhost:8080`
   - Login with credentials `admin@example.com/airflow`
4. **Trigger and Monitor DAGs** ğŸ¯
   - Activate and trigger DAGs in the Airflow UI.

## ğŸš€ Future Enhancements
- ğŸ”¹ Implement CI/CD for automated deployments.
- ğŸ”¹ Add data validation and monitoring.
- ğŸ”¹ Extend support for cloud data warehouses (e.g., Snowflake, BigQuery).

## ğŸ‘¤ Author
**Aditya Rathore**

## ğŸ”— Connect with Me
- **GitHub**: [Aditya Rathore](https://github.com/adityarathore622)
- **LinkedIn**: [Aditya RAthore](https://linkedin.com/in/your-profile)

---
ğŸŒŸ *Feel free to contribute or raise issues in the repository!* 


